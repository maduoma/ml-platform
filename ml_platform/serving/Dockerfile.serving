# ml_platform/serving/Dockerfile.serving
# This Dockerfile creates a container image for the KServe model server.
# It uses a multi-stage build to create a lean and secure final image.

# --- Build Stage ---
# This stage installs dependencies into a virtual environment.
FROM python:3.9-slim as builder

WORKDIR /app

# Install poetry for dependency management
RUN pip install poetry==1.5.1

# Copy only the dependency definition files
COPY poetry.lock pyproject.toml ./

# Install dependencies into a virtual environment within the image
# --no-dev: Excludes development dependencies (like pytest)
# --no-root: Skips installing the project itself, as we'll copy the source later
RUN poetry config virtualenvs.in-project true && \
    poetry install --no-dev --no-root


# --- Final Stage ---
# This stage creates the final, lean production image.
FROM python:3.9-slim

WORKDIR /app

# Copy the virtual environment with pre-installed dependencies from the builder stage
COPY --from=builder /app/.venv ./.venv

# Activate the virtual environment for subsequent RUN/CMD commands
ENV PATH="/app/.venv/bin:$PATH"

# Copy the application source code
COPY ml_platform/serving/ /app/

# In a real-world scenario, the trained model would be downloaded from a
# model registry (like MLflow or S3) during the container's startup command
# or by an init container. For this project, we copy the dummy model.
COPY notebooks/dummy_model.joblib /mnt/models/model.joblib

# Set environment variables that KServe uses to find the model
ENV MODEL_NAME="music-recommender"
ENV MODEL_PATH="/mnt/models/model.joblib"

# The command that starts the KServe model server.
# This will run the music_recommender_predictor.py script.
CMD ["python", "-m", "ml_platform.serving.predictors.music_recommender_predictor"]